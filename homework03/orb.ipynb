{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd9e66d",
   "metadata": {},
   "source": [
    "# ORB Homework - Helper Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc681bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacee818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from scipy import ndimage as ndi\n",
    "from utils import apply_gaussian_2d\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062aa8ce",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "\n",
    "**ORB** pipeline is extremely well-known and is implemented in many packages available to you. Before we try to implement the detector ourselves, let's see how it looks in the readily-available OpenCV implementation. We first take the ORB feature detector and, independently for each image, detect reproducible and discriminative regions of interest. This is exactly the part of the pipeline which we will try to re-implement in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c95cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the example image and create its transformed version to test correspondence mathcing\n",
    "img1 = cv2.imread('img/face1.jpg')\n",
    "M = np.array([[1, abs(0.3), 0],[0,1,0]])\n",
    "nW =  img1.shape[1] + abs(0.3 * img1.shape[0])\n",
    "img2 = cv2.warpAffine(img1, M, (int(nW), img1.shape[0]))\n",
    "(h, w) = img2.shape[:2]\n",
    "(cX, cY) = (w // 2, h // 2)\n",
    "M = cv2.getRotationMatrix2D((cX, cY), 30, 1.0)\n",
    "img2 = cv2.warpAffine(img2, M, (w, h))\n",
    "\n",
    "# Detect keypoints with ORB -> THIS IS THE PART WE WILL TRY TO RE-IMPLEMENT\n",
    "orb_opencv = cv2.ORB_create()\n",
    "kp1 = orb_opencv.detect(img1, None)\n",
    "kp2 = orb_opencv.detect(img2, None)\n",
    "\n",
    "# Visualize the results\n",
    "img1kp = cv2.drawKeypoints(img1, kp1, None, color=(0, 255, 0))\n",
    "img2kp = cv2.drawKeypoints(img2, kp2, None, color=(0, 255, 0))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(25, 8))\n",
    "axs[0].imshow(cv2.cvtColor(img1kp, cv2.COLOR_RGB2BGR))\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(cv2.cvtColor(img2kp, cv2.COLOR_RGB2BGR))\n",
    "axs[1].axis(\"off\")\n",
    "fig.suptitle(\"Features detected independently on a pair of images\", size=26);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f473d0",
   "metadata": {},
   "source": [
    "Once we have the interest points for a pair of images, we can match and filter them to create a set of correspondences, which can be further used in different computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e55bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_opencv = cv2.ORB_create()\n",
    "kp1, desc1 = orb_opencv.detectAndCompute(img1, None)\n",
    "kp2, desc2 = orb_opencv.detectAndCompute(img2, None)\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.match(desc1, desc2) \n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches[:30], None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "fig, ax = plt.subplots(figsize=(35, 10))\n",
    "ax.imshow(cv2.cvtColor(res, cv2.COLOR_RGB2BGR))\n",
    "ax.axis(\"off\")\n",
    "fig.suptitle(\"Matched features\", size=26);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a72dac",
   "metadata": {},
   "source": [
    "## Individual Functions Helpers\n",
    "\n",
    "Below you will find the example outputs you should get for each functions so that you can compare your results to it.\n",
    "\n",
    "### `get_first_test_mask()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orb\n",
    "\n",
    "def mask2d_to_cv2kp(mask,kp_size=1):\n",
    "    cols,rows = np.where(mask)\n",
    "    return tuple([\n",
    "        cv2.KeyPoint(float(r),float(c),kp_size) for c,r \n",
    "        in zip(list(cols),list(rows))\n",
    "    ])\n",
    "\n",
    "# Visualize the first mask\n",
    "treshold = 20\n",
    "border = 3\n",
    "\n",
    "first_mask1 = orb.get_first_test_mask(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY).astype(int), threshold=treshold,border=border)\n",
    "first_mask2 = orb.get_first_test_mask(cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY).astype(int), threshold=treshold,border=border)\n",
    "kp0_1 = mask2d_to_cv2kp(first_mask1)\n",
    "kp0_2 = mask2d_to_cv2kp(first_mask2)\n",
    "\n",
    "# Visualize the results\n",
    "img1kp = cv2.drawKeypoints(img1, kp0_1, None, color=(0, 255, 0))\n",
    "img2kp = cv2.drawKeypoints(img2, kp0_2, None, color=(0, 255, 0))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(25, 8))\n",
    "axs[0].imshow(cv2.cvtColor(img1kp, cv2.COLOR_RGB2BGR))\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(cv2.cvtColor(img2kp, cv2.COLOR_RGB2BGR))\n",
    "axs[1].axis(\"off\")\n",
    "fig.suptitle(\"Features detected independently on a pair of images\", size=26);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f30ba2",
   "metadata": {},
   "source": [
    "### `detect_keypoints()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9932e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orb\n",
    "\n",
    "kp1, scores1 = orb.detect_keypoints(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), threshold=20, border=10)\n",
    "kp2, scores2 = orb.detect_keypoints(cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY), threshold=20, border=10)\n",
    "\n",
    "kp1, kp2 = np.asarray(kp1), np.asarray(kp2)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(35, 10))\n",
    "axs[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "sc1 = axs[0].scatter(kp1[:, 1], kp1[:, 0], c=scores1, cmap=\"Greens\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "sc2 = axs[1].scatter(kp2[:, 1], kp2[:, 0], c=scores2, cmap=\"Greens\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.colorbar(sc1, ax=axs[0])\n",
    "plt.colorbar(sc2, ax=axs[1])\n",
    "fig.suptitle(\"Features detected independently on a pair of images (colored by corresponding score)\", size=26);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc005c",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/img_kp_score_ref.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d13e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(35, 10))\n",
    "axs[0].plot(scores1, c=\"red\", linewidth=2.0)\n",
    "axs[1].plot(scores2, c=\"red\", linewidth=2.0)\n",
    "for i in [0, 1]:\n",
    "    axs[i].set_xlabel(\"keypoints index\", fontsize=20)\n",
    "    axs[i].set_ylabel(\"keypoint score\", fontsize=20)\n",
    "fig.suptitle(\"Scores of initial FAST features\", size=26);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48432411",
   "metadata": {},
   "source": [
    "### `create_pyramid()`\n",
    "\n",
    "**Note the changed image sizes on the x and y axes!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orb\n",
    "\n",
    "for downscale_factor in [1.5, 2.0, 3.0]:\n",
    "    pyr = orb.create_pyramid(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), n_pyr_layers=4, downscale_factor=downscale_factor)\n",
    "    fig, axs = plt.subplots(1, len(pyr), figsize=(15, 5))\n",
    "    for i in range(len(pyr)):\n",
    "        axs[i].imshow(pyr[i], cmap=\"gray\")\n",
    "        for tick in axs[i].xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16) \n",
    "            tick.label.set_rotation('vertical')\n",
    "        for tick in axs[i].yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16)\n",
    "    fig.suptitle(f\"Image pyramid with 4 levels and downscaling factor {downscale_factor}\", size=26)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  \n",
    "pyr = [\n",
    "    orb.create_pyramid(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), n_pyr_layers=4, downscale_factor=downscale_factor)\n",
    "    for downscale_factor\n",
    "    in [1.5, 2.0, 3.0]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e37ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(np.isclose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540c6bc",
   "metadata": {},
   "source": [
    "### `get_x_derivative()` and `get_y_derivative()`\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/sobel.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orb\n",
    "\n",
    "dx = orb.get_x_derivative(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY))\n",
    "dy = orb.get_y_derivative(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "im1 = axs[0].imshow(dx, cmap=\"gray\")\n",
    "axs[0].axis(\"off\")\n",
    "im2 = axs[1].imshow(dy, cmap=\"gray\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.colorbar(im1, ax=axs[0])\n",
    "plt.colorbar(im2, ax=axs[1])\n",
    "fig.suptitle(\"Derivatives in x and y directions for the first image\", size=26);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988657b",
   "metadata": {},
   "source": [
    "Correct results:\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/img_x_y_derivative.jpg\"><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73796368",
   "metadata": {},
   "source": [
    "### `get_harris_response()`\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/structure_tensor.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orb\n",
    "\n",
    "harris1 = orb.get_harris_response(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY))\n",
    "harris2 = orb.get_harris_response(cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "im1 = axs[0].imshow(harris1, cmap=\"jet\")\n",
    "axs[0].axis(\"off\")\n",
    "im2 = axs[1].imshow(harris2, cmap=\"jet\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.colorbar(im1, ax=axs[0])\n",
    "plt.colorbar(im2, ax=axs[1])\n",
    "fig.suptitle(\"Harris response computed for both example images\", size=26);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd90697",
   "metadata": {},
   "source": [
    "### `filter_keypoints()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dac7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orb\n",
    "\n",
    "gray1, gray2 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "kp1, scores1 = orb.detect_keypoints(gray1, threshold=20, border=10)\n",
    "kp2, scores2 = orb.detect_keypoints(gray2, threshold=20, border=10)\n",
    "n_max_level = 100\n",
    "kp1, kp2 = np.asarray(kp1), np.asarray(kp2)\n",
    "\n",
    "# Some pre-filtering by score: already implemented for you in orb.fast()\n",
    "idxs1, idxs2 = np.argsort(scores1)[::-1], np.argsort(scores2)[::-1]\n",
    "kp1 = kp1[idxs1][: 2 * n_max_level]\n",
    "kp2 = kp2[idxs2][: 2 * n_max_level]\n",
    "scores1 = np.asarray(scores1)[idxs1][: 2 * n_max_level]\n",
    "scores2 = np.asarray(scores2)[idxs2][: 2 * n_max_level]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(35, 10))\n",
    "axs[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "sc1 = axs[0].scatter(kp1[:, 1], kp1[:, 0], c=scores1, cmap=\"Greens\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "sc2 = axs[1].scatter(kp2[:, 1], kp2[:, 0], c=scores2, cmap=\"Greens\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.colorbar(sc1, ax=axs[0])\n",
    "plt.colorbar(sc2, ax=axs[1])\n",
    "fig.suptitle(\"Pre-filtered features (colored by corresponding score)\", size=26);\n",
    "\n",
    "kp1 = orb.filter_keypoints(gray1, kp1.tolist(), n_max_level)\n",
    "kp2 = orb.filter_keypoints(gray2, kp2.tolist(), n_max_level)\n",
    "harris1, harris2 = orb.get_harris_response(gray1), orb.get_harris_response(gray2)\n",
    "kp1_responses = [harris1[row_idx, col_idx] for row_idx, col_idx in kp1][:len(kp1)]\n",
    "kp2_responses = [harris2[row_idx, col_idx] for row_idx, col_idx in kp2][:len(kp2)]\n",
    "kp1, kp2 = np.asarray(kp1), np.asarray(kp2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "sc1 = axs[0].scatter(kp1[:, 1], kp1[:, 0], c=kp1_responses, cmap=\"Reds\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "sc2 = axs[1].scatter(kp2[:, 1], kp2[:, 0], c=kp2_responses, cmap=\"Reds\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.colorbar(sc1, ax=axs[0])\n",
    "plt.colorbar(sc2, ax=axs[1])\n",
    "fig.suptitle(\"Features filtered by the Harris responses (colored by Harris response)\", size=26);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec488d0",
   "metadata": {},
   "source": [
    "Correct results\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/img_filtered_kp_score_ref.jpg\"><br>\n",
    "  <img src=\"img/img_filtered_kp_hresp_ref.jpg\"><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90759df0",
   "metadata": {},
   "source": [
    "### `fast()`\n",
    "\n",
    "(Already implemented for you in `orb.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orb\n",
    "\n",
    "gray1, gray2 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "kp1, kp2 = orb.fast(gray1), orb.fast(gray2)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "colors = cm.get_cmap('Reds')(np.linspace(0, 1, len(kp1)))\n",
    "axs[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "axs[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "for level, (kp1_level, kp2_level) in enumerate(zip(kp1, kp2)):\n",
    "    kp1_level, kp2_level = np.asarray(kp1_level), np.asarray(kp2_level)\n",
    "    axs[0].scatter(kp1_level[:, 1], kp1_level[:, 0], color=colors[level], alpha=0.5)\n",
    "    axs[1].scatter(kp2_level[:, 1], kp2_level[:, 0], color=colors[level], alpha=0.5)\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].axis(\"off\")\n",
    "fig.suptitle(\"Features filtered by the Harris responses (colored by pyramid level)\", size=26);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a489cfa07953854e7286b0bbb213f4258b91956e33be911d3c4faf838544128e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
